# ğŸ§  Improving Chatbot Accuracy using Context Window Optimization

This repository accompanies the research paper:
> **"Improving Chatbot Accuracy using Context Window Optimization"**

> *Author:* Gaurav Kumar, Sagar Singh

> *Affiliation:* Independent Researchers, India

> *Email:* gauravatwork17@gmail.com, sagarsinghatwork@gmail.com 
---

## ğŸ§© Overview

Large conversational language models perform better when they receive **the right context**, not necessarily *more* context.  
This project explores **context window optimization** techniques to improve chatbot accuracy, efficiency, and cost.

We compare:

1. **Full Context** (baseline)  
2. **Sliding Window** (keep last *k* turns)  
3. **Relevance-based Pruning** (select only the most relevant turns using embeddings)

---

## ğŸš€ Project Goals

- Reduce unnecessary token usage while maintaining or improving accuracy.  
- Compare traditional full-context input vs optimized context selection.  
- Evaluate with automatic metrics (F1, EM, ROUGE) and human scores (factuality, relevance, fluency).  
- Provide a **reproducible open-source framework** for further experiments.  

---

## âš™ï¸ System Architecture

**Workflow:**

1. User sends query `q`.  
2. System embeds query + past conversation turns.  
3. Calculates cosine similarity for relevance.  
4. Selects optimal subset of context using threshold `S` and delta pruning.  
5. Sends optimized prompt to LLM (GPT-3.5 or Llama2).  
6. Evaluates response accuracy, token cost, and speed.

ğŸ“Š The figure below shows how pruning affects accuracy and efficiency.

### Figures (Generated Example)

| Figure | Description |
|--------|-------------|
| ![F1 Scores](figures/f1_scores.png) | F1 Score by Method |
| ![Avg Tokens](figures/avg_tokens.png) | Average Tokens in Prompt |
| ![F1 vs Tokens](figures/f1_vs_tokens.png) | Trade-off between Accuracy and Tokens |

---

## ğŸ“ Repository Structure

```
chatbot-context-opt/
â”œâ”€â”€ paper.docx                     # Research manuscript
â”œâ”€â”€ code/                          # Core scripts
â”‚   â”œâ”€â”€ embeddings.py              # Sentence embedding module (SBERT)
â”‚   â”œâ”€â”€ context_selector.py        # Context selection logic
â”‚   â”œâ”€â”€ run_experiments.py         # Main experiment runner
â”‚   â”œâ”€â”€ plot_results.py            # Generate result plots
â”‚   â””â”€â”€ fill_results.py            # Auto-insert results into .docx
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_results.csv         # Example evaluation metrics
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ f1_scores.png
â”‚   â”œâ”€â”€ avg_tokens.png
â”‚   â””â”€â”€ f1_vs_tokens.png
â”œâ”€â”€ requirements.txt               # Required Python libraries
â”œâ”€â”€ README.md                      # You are reading this
â”œâ”€â”€ LICENSE                        # MIT License
â””â”€â”€ .gitignore
```

---

## ğŸ§  How to Run Experiments

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Prepare your data
- **Option A:** Create your own synthetic chat dataset.  
- **Option B:** Use public QA datasets like CoQA or MultiWoZ.

### 3ï¸âƒ£ Embed and select context
```python
from embeddings import embed_texts
from context_selector import select_by_threshold
```

### 4ï¸âƒ£ Run your tests
```bash
python code/run_experiments.py
```

### 5ï¸âƒ£ View results
Results will be saved in `/data/sample_results.csv`.

### 6ï¸âƒ£ Generate figures
```bash
python code/plot_results.py data/sample_results.csv
```

### 7ï¸âƒ£ Update paper with results
```bash
python code/fill_results.py improving_chatbot_context_optimization.docx data/sample_results.csv
```

---

## ğŸ“Š Example Results (Synthetic Data)

| Method | EM   | F1   | ROUGE-L | Avg Tokens |
|--------|------|------|---------|------------|
| Full-context | 0.60 | 0.62 | 0.10 | 2762 |
| Sliding (k=3) | 0.62 | 0.64 | 0.11 | 1500 |
| Relevance-Prune (S=0.6) | 0.66 | 0.68 | 0.12 | 2253 |

---

## ğŸ§ª Planned Experiments

| Goal | Metric | Expected Outcome |
|------|--------|-----------------|
| Reduce prompt length | Avg Tokens | â†“ 30â€“50% |
| Maintain accuracy | F1 / EM | â‰¥ Baseline |
| Improve relevance | Human score | â†‘ 10% |
| Cost reduction | Token usage | â†“ 25% |

---

## ğŸ“˜ Publication Info

- Category: cs.CL (Computational Linguistics) or cs.AI  
- License: CC BY 4.0  
- Expected Upload: [Insert date]  

**Citation (once published):**
```
Gaurav Kumar, "Improving Chatbot Accuracy using Context Window Optimization", arXiv:XXXX.XXXXX, 2025
```

---

## ğŸ“š References

- Lewis, P. et al. â€œRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.â€ NeurIPS (2020)  
- Reimers, N. & Gurevych, I. â€œSentence-BERT: Sentence Embeddings using Siamese BERT-Networks.â€ EMNLP (2019)  
- OpenAI GPT API Documentation  
- LangChain Framework  

---

## ğŸ§¾ License

This project is licensed under the MIT License â€“ feel free to use and modify with attribution.

---

## âœ¨ Acknowledgements

Special thanks to ChatGPT (OpenAI) for research assistance, pseudocode generation, and report structuring support.

---

## ğŸ“¬ Contact

For questions, collaborations, or implementation help:  
**Gaurav Kumar** â€” [gauravatwork17@gmail.com]

**LinkedIn:** [Gaurav K](https://www.linkedin.com/in/gauravconnects/) 


**Sagar Singh** â€” [sagarsinghatwork@gmail.com]

**LinkedIn:** [Sagar S](https://www.linkedin.com/in/sagar--singh/) 

